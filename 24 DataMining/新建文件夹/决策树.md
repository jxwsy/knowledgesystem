# 决策树

	https://blog.csdn.net/HerosOfEarth/article/details/52347820

## 熵

**自信息量**：一个事件（消息）本身所包含的信息量。随机事件Xi发生概率为P(xi)，则随机事件的自信息量定义为：

![](https://i.imgur.com/FCPhorr.jpg)

**信息熵**：随机变量自信息量I(xi)的数学期望。表示随机变量不确定性的度量，熵越大，随机变量的不确定性就越大。

![](https://i.imgur.com/9sdo4Fi.jpg)


**条件熵**（conditional entropy）：在已知随机变量X的条件下随机变量Y的不确定性。

![](https://i.imgur.com/3LdHjp0.jpg)

![](https://i.imgur.com/Hc0sJGq.jpg)

**信息增益**（information gain）：表示已知特征X的信息而使得类Y的信息的不确定性减少的程度。特征A对训练数据集D的信息增益g(D,A)，定义为D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即

![](https://i.imgur.com/a2Ezq7e.jpg)

![](https://i.imgur.com/kYZB374.jpg)

**信息增益比**（information gain ratio）：

![](https://i.imgur.com/5iGAyz3.jpg)

**基尼指数**(gini index):度量数据集D的不纯度。

![](https://i.imgur.com/KI0BQam.jpg)


## 决策树 

内部节点(非叶结点)：表示在一个属性上的测试

叶节点：存放类标号。

根节点

![](https://i.imgur.com/3OQSULP.jpg)

![](https://i.imgur.com/MMUuwRR.jpg)

以信息增益作为划分训练数据集的特征的算法称为ID3算法.（容易产生过度匹配的情况）

以信息增益率作为划分训练数据集的特征的算法称为C4.5算法.

以基尼指数作为划分训练数据集的特征的算法称为Cart算法.

============================================================

![](https://i.imgur.com/I6MefBS.jpg)

剪枝分预剪枝和后剪枝。

![](https://i.imgur.com/1ecSe73.jpg)
![](https://i.imgur.com/Y08q94M.jpg)

如何判断是否有提升？采用留出法，即留出一部分数据作为验证集，进行评估。

============================================================

ID3算法的问题：

	1.C4.5避免了ID3算法中的归纳偏置问题，因为ID3算法会偏向于选择类别较多的属性（形成分支较多会导致信息增益大）。
	2.C4.5的改进就是可以处理连续值（二元切分）
		首先对于连续值属性的值进行排序(A1,A2......An);
		然后我们可以在每两个值之间取一个点，用这个点就可以把该组连续值分为两部分，比如我们去一个点a1($A1<a1<A2$),
		则小于a1的部分（A1）大于a1的部分（A2......An）。但是这个a1好吗？不知道呀！那我们loop一下所有这样的a(共有n-1个)，
		每一个ai我们可以算出分裂前后的信息增益率，然后我们求一个max即可。
	3.ID3无法处理空值。
		每一个特征的取值都有若干个，根据训练集每个可能的取值都有一个概率，我们用这个概率来表示这个确实值得可能取值。

============================================================

cart:

不同于 ID3 与 C4.5, CART 为一种二分决策树， 每次对特征进行切分后只会产生两个子节点，而ID3 或 C4.5 中决策树的分支

是根据选定特征的取值来的，切分特征有多少种不同取值，就有多少个子节点（连续特征进行离散化即可）。CART 设计回归与分类。

https://www.cnblogs.com/ooon/p/5647309.html

https://blog.csdn.net/gzj_1101/article/details/78355234