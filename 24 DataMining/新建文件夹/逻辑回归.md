# 逻辑回归

在分类问题中，要预测的变量y是离散的值。例如，判断一封电子邮件是否是垃圾邮件;判断一次金融交易是否是欺诈。逻辑回归算法是分类算法。

线性回归中，设定阈值为0.5，那么

![](https://i.imgur.com/3uzUkVl.png)

假设函数的输出值可能远大于 1，或者远小于 0。

![](https://i.imgur.com/Diqf4zL.png)

观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的 训练集中来，这将使得我们获得一条新的直线。这时，再使用 0.5 作为阀值来预测肿瘤是良性还是恶性便不合适了。可以看出，线性回 归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决这样的问题。

我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在 0 和 1 之间。 

s型函数：

![](https://i.imgur.com/Vu9KuBw.png)

![](https://i.imgur.com/n612tEI.png)

逻辑回归模型的假设函数是

![](https://i.imgur.com/edkXuU8.png)

![](https://i.imgur.com/XzYZGiC.png)

hθ(x)的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1 的概率。

例如，如果对于给定的 x，通过已经确定的参数计算得出 hθ(x)=0.7，则表示有 70%的几 率 y 为正向类，相应地 y 为负向类的几率为 1-0.7=0.3。

线性回归模型中，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义。但是我们得到的代价函数将是一个非凸函数，这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

所以：

![](https://i.imgur.com/Q0FdixI.png)

代价函数为：

![](https://i.imgur.com/PyVhl8f.png)

![](https://i.imgur.com/PGtgRh1.png)

![](https://i.imgur.com/CmFqITl.png)

![](https://i.imgur.com/SRxnher.png)

这样构建的 Cost(hθ(x),y)函数的特点是:当实际的 y=1 且 hθ 也为 1 时误差为 0，当 y=1 但 hθ 不为 1 时误差随着 hθ 的变小而变大;当实际的 y=0 且 hθ 也为 0 时代价为 0，当 y=0 但 hθ 不为 0 时误差随着 hθ 的变大而变大。

在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。同时更新所有 θ 的值。算法为:

![](https://i.imgur.com/NrtYLsz.png)

在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。

除了梯度下降算法以外，还有一些常被用来令代价函 数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下 降算法要更加快速。这些算法有:共轭梯度(Conjugate Gradient)，局部优化法(Broyden fletcher goldfarb shann,BFGS)和有限内存局部优化法(LBFGS)

## 决策边界

见视频

## 多类别分类

假设数据集存在3个类别ABC，将A作为一类，其余作为一类，训练一个标准的逻辑回归分类器h1，再将B作为一类，其余作为一类，训练一个标准的逻辑回归分类器h2，以此类推，最后我们得到一系列的模型简记为:

![](https://i.imgur.com/8bETJoZ.png)

![](https://i.imgur.com/zFxDAJ2.png)

最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量， 都选择最高概率的输出变量。

## 正则化

对回归问题：

![](https://i.imgur.com/GZTyqvu.png)

第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集;第三个模型是一 个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质:预测新数据。我们可以看 出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的 训练集但在新输入变量进行预测时可能会效果不好;而中间的模型似乎最合适。

对分类：

![](https://i.imgur.com/B5WSM0V.png)

就以多项式理解，x 的次数越高，拟合的越好，但相应的预测的能力就可能变差。 问题是，如果我们发现了过拟合问题，应该如何处理?

1. 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙(例如 PCA)

2. 正则化。 保留所有的特征，但是减少参数的大小(magnitude)。

那么代价函数为：

![](https://i.imgur.com/sN50jUZ.png)

其中 λ 又称为正则化参数(Regularization Parameter)。 注:根据惯例，我们不对θ0进行惩罚。

因为如果我们令λ的值很大的话，为了使 Cost Function 尽可能的小，所有的θ的值(不包括θ0)都会在一定程度上减小。

但若λ的值太大了，那么 θ(不包括 θ0)都会趋近于 0，这样我们所得到的只能是一条平行于x轴的直线。 所以对于正则化，我们要取一个合理的λ的值，这样才能更好的应用正则化。

## 正则化线性回归

正则化线性回归的代价函数为:

![](https://i.imgur.com/4ipH6kO.png)

如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对 θ0 进行正则化，所 以梯度下降算法将分两种情形:

![](https://i.imgur.com/TBkkcVS.png)

对上面的算法中 j=1,2,...,n 时的更新式子进行调整可得:

![](https://i.imgur.com/FwVz7hK.png)

可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的 基础上令 θ 值减少了一个额外的值。

我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示:

![](https://i.imgur.com/w6OVI4T.png)

图中的矩阵尺寸为 (n+1)*(n+1)。

## 正则化的逻辑回归模型

![](https://i.imgur.com/zuK3p2u.png)

![](https://i.imgur.com/V0VWmtt.png)

![](https://i.imgur.com/s3VZbTQ.png)