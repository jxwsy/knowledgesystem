# 正则化
## 范数

范数是具有“长度”概念的函数。

#### 向量范数

对公式![](https://i.imgur.com/oeS8YE2.png)

![](https://i.imgur.com/gp7HQ3q.png)

![](https://i.imgur.com/0NKFkmY.png)

- **p-范数**

![](https://i.imgur.com/hZpncC1.png)

这个p和之前的q从是一个东西，随着p越大，等高线图越接近正方形（正无穷范数）；越小，曲线弯曲越接近原点（负无穷范数）

- **−∞-范数**

![](https://i.imgur.com/kZCWTo9.png)

所有向量元素中绝对值的最小值

- **∞-范数**

![](https://i.imgur.com/hSdMX2u.png)

所有向量元素中绝对值的最大值，也称棋盘距离（chessboard），切比雪夫距离

- **0-范数**

向量中非零元素的数量

- **1-范数**

![](https://i.imgur.com/DM9f9oy.png)

向量元素绝对值之和，也称街区距离。

- **2-范数**

![](https://i.imgur.com/4ZyHTCy.png)

欧氏距离

#### 矩阵范数

	https://charlesliuyx.github.io/2017/10/03/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E4%BB%80%E4%B9%88%E6%98%AF%E6%AD%A3%E5%88%99%E5%8C%96/
	
## 正则化

	https://www.zhihu.com/question/20473040

正则化是为了防止过拟合， 进而增强泛化能力。

正则化(Regularization):削弱对预测结果的影响。

	Keep all the feature and reduce Magnitude/values of parameters.

一般正则项是以下公式的形式：

![](https://i.imgur.com/XXCrKrm.png)

M是模型的阶次（表现形式是数据的维度），比如M=2，就是一个平面（二维）内的点。

![](https://i.imgur.com/Bh97foy.png)

上图中，蓝色的圆圈表示原问题可能的解范围，橘色的表示正则项可能的解范围。而整个目标函数（原问题+正则项）有解当且仅当两个解范围相切。从上图可以很容易地看出，由于2范数解范围是圆，所以相切的点有很大可能不在坐标轴上（感谢评论区＠临熙指出表述错误），而由于1范数是菱形（顶点是凸出来的），其相切的点更可能在坐标轴上，而坐标轴上的点有一个特点，其只有一个坐标分量不为零，其他坐标分量为零，即是稀疏的。所以有如下结论，1范数可以导致稀疏解，2范数导致稠密解。那么为什么不用0范数呢，理论上它是求稀疏解最好的规范项了。然而在机器学习中，特征的维度往往很大，解0范数又是NP-hard问题，所以在实际中不可行。但是用1范数解是可行的，并且也可以得到稀疏解，所以实际稀疏模型中用1范数约束。

