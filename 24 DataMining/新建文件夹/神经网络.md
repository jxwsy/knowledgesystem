# 神经网络 

无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，
计算的负荷会非常大。

模拟人体大脑。

	树突----->神经元----->轴突
	输入----->激活单元----->输出

以逻辑回归模型作为自身学习模型的神经元示例

![](https://i.imgur.com/ReZdOBv.jpg)

上图为一个3层的神经网络，第一层成为输入层（Input Layer），最后一
层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）。

![](https://i.imgur.com/o0I5akY.jpg)


对于上图所示的模型，激活单元和输出分别表达为

![](https://i.imgur.com/dpmFlJ2.jpg)

**前向传播算法**：每一个a都是由上一层所有的x和每一个x所对应的权重决定。

从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征 x 1 ,x 2 ,...,x n ，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。

在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为**第二层中的特征是神经网络通过学习后自己得出的
一系列用于预测输出变量的新特征。**

## 向量化

![](https://i.imgur.com/qRyBP6d.jpg)

- 将模型向量化；
- 用z表示函数g的自变量；
- z的上标指的是计算第几层的值；
- X为训练集特征矩阵，每行为一条训练实例；

## AND、OR、NOT

当输入特征为布尔值（0 或 1）时，我们可以用一个单一的激活层(无中间层)可以作为二元逻辑运算符，为了表示不同的运算符，我们之需要
选择不同的权重即可。

![](https://i.imgur.com/Rz95p8Q.jpg)

##  多类分类

如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有 4 个值（有几类就有几个输出）。例如，第一个值为 1 或 0 用于预测是否是行人，第二个值用于判断是否为汽车。

输入向量 x 有三个维度，两个中间层，输出层 4 个神经元分别用来表示 4 类，也就是每一个数据在输出层都会出现[a b c d] T ，且 a,b,c,d 中**仅有一个为 1**，表示当前类。下面是该神经网络的可能结构示例：

![](https://i.imgur.com/yVILX6m.jpg)
 

## 代价函数

- 神经网络的训练样本有 m 个，每个包含一组输入 x 和一组输出信号 y；
- L 表示神经网络层数；
- Sl表示第l层的 neuron 个数（不包含偏置单元）；
- SL代表最后一层中处理单元的个数。

![](https://i.imgur.com/zcTCDms.jpg)

hθ(x)是一个维度为 K 的向量，其代价函数为：

![](https://i.imgur.com/UPgGdnj.jpg)

归一化的那一项只是排除了每一层 θ0后，每一层的θ矩阵的和。最里层的循环j循环所有的行（由 sl+1层的激活单元数决定），循环 i 则循环所有的列，由该层（sl层）的激活单元数所决定。即：hθ(x)与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization 的 bias 项处理所有参数的平方和。

## 反向传播算法

- 复合函数的链式法则
- 反向传播，就是计算梯度的方法

![](https://i.imgur.com/A4rd6Gt.jpg)

	http://www.cnblogs.com/charlotte77/p/5629865.html
	https://www.zhihu.com/question/27239198?rf=24827633