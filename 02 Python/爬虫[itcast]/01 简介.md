# 爬虫基础

## 1 数据获取的方式

    (1)企业生产的用户数据：

    (2)数据管理咨询公司：通过市场调研、问卷调查、固定的样本检测，和各行各业的公司进行合作、专家对话（数据积累很多年了，最后得出科研结果）来采集数据。

    (3)政府/机构提供的公开数据

    (4)第三方数据平台购买数据

    (5)爬虫爬取数据。

## 2 什么是爬虫

抓取网页数据的程序。

## 3 爬虫怎么抓取网页数据

网页三大特征：

    -网页都有自己唯一的URL（统一资源定位符）来进行定位
    -网页都使用HTML （超文本标记语言）来描述页面信息。
    -网页都使用HTTP/HTTPS（超文本传输协议）协议来传输HTML数据。

爬虫的设计思路：

    -首先确定需要爬取的网页URL地址。
    -通过HTTP/HTTP协议来获取对应的HTML页面。
    -提取HTML页面里有用的数据：
        a. 如果是需要的数据，就保存起来。
        b. 如果是页面里的其他URL，那就继续执行第二步。


## 4 为什么选择Python做爬虫？

可以做爬虫的语言有很多，如 PHP、Java、C/C++、Python等等...

    - PHP 对多线程、异步支持不够好，并发处理能力很弱。
          爬虫是工具性程序，对速度和效率要求比较高。

    - Java 语言本身很笨重，代码量很大。重构成本比较高，
           任何修改都会导致代码的大量变动。而爬虫经常需要修改部分采集代码。

    - C/C++ 运行效率和性能几乎最强，但是学习成本很高，代码成型比较慢。
            能用C/C++做爬虫，但不是正确的选择。

    - Python 语法优美、代码简洁、开发效率高、支持的模块多，相关的HTTP请求模块和HTML解析模块非常丰富。
             强大的爬虫Scrapy，及成熟高效的 scrapy-redis分布式策略。
             调用其他借口也非常方便（胶水语言）

## 5 爬虫分类

根据使用场景：分为 通用爬虫 和 聚焦爬虫


### 通用爬虫

搜索引擎用的爬虫系统。

(1)目标：尽可能把互联网上所有的网页下载下来，放到本地服务器里形成备份，再对这些网页做相关处理（提取关键字、去掉广告），最后提供一个用户检索接口。

(2)抓取流程：

    a) 首选选取一部分已有的URL，把这些URL放到待爬取队列。
    b) 从队列里取出这些URL，然后解析DNS得到主机IP，
       然后去这个IP对应的服务器里下载HTML页面，保存到搜索引擎的本地服务器。
       之后把这个爬过的URL放入已爬取队列。
    c) 分析这些网页内容，找出网页里其他的URL连接，继续执行第二步，直到爬取条件结束。

(3)搜索引擎如何获取一个新网站的URL：

    -主动向搜索引擎提交网址`http://zhanzhang.baidu.com/linksubmit/url`
    -在其他网站里设置网站的外链。
    -搜索引擎会和DNS服务商进行合作，可以快速收录新的网站。

    DNS：就是把域名解析成IP的一种技术。

(4)通用爬虫并不是万物皆可爬，它也需要遵守规则：
Robots协议会指明通用爬虫可以爬取网页的权限。
Robots.txt 只是一个建议。并不是所有爬虫都遵守，一般只有大型的搜索引擎爬虫才会遵守。


(5)通用爬虫工作流程

    爬取网页 - 存储数据 - 内容处理 - 提供检索/排名服务

(6)搜索引擎排名

    -PageRank值：根据网站的流量（点击量/浏览量/人气）统计，流量越高，网站也越值钱，排名越靠前。
    -竞价排名：谁给钱多，谁排名就高。

(7)通用爬虫的缺点

    -只能提供和文本相关的内容（HTML、Word、PDF）等等，但是不能提供多媒体文件（音乐、图片、视频）和二进制文件（程序、脚本）等等。
    -提供的结果千篇一律，不能针对不同背景领域的人提供不同的搜索结果。
    -不能理解人类语义上的检索。

### 聚焦爬虫

为了解决以上问题，聚焦爬虫出现了：

聚焦爬虫：爬虫程序员写的针对某种内容的爬虫。

面向主题爬虫，面向需求爬虫：会针对某种特定的内容去爬取信息，而且会保证信息和需求尽可能相关。
